# -*- coding: utf-8 -*-
"""PubmedSample.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZxzZQpezlz-4cKB4nrIgYahsUpiqhYMO

Code to get all PMIDs for journal articles after 2014
"""

import requests

#list where PMIDs will be dumped
ids = []
start_index = 0
max_count = 100

# get PMIDs for relevant articles (after 2014, journal articles only)

while start_index < max_count:
		response = requests.get(f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?dm=pubmed&datetype=pdat&mindate=2015&maxdate=2022&term="journal+article"&field=Publication+Type&retmode=json&retmax=100000&retstart={start_index}')
		print(response.json())
		ids.extend(response.json()['esearchresult']['idlist'])
		max_count = int(response.json()['esearchresult']['count'])
		print(f"fetched {len(ids)} ids")
		start_index += 100000
		print(start_index)

print(ids)

with open('all_pmids.txt', 'w') as f:
    for item in ids:
        f.write("%s\n" % item)

"""Code to pull a random sample from the full list of PMIDs and extract the first affilitation field from each one"""

from random import sample
#from collections.abc import MutableMapping
from requests_throttler import BaseThrottler
from bs4 import BeautifulSoup as BS
import csv, requests, lxml
from ratelimiter import RateLimiter




#read list of all pmids from .txt
all_pmids = []

with open('all_pmids.txt') as f:
	for line in f:
		all_pmids.append(line.strip())

#set up 
all_tested_ids = []
exception_count = 0
repeats = []
no_affiliation = []

#test each randomly selected pmid against criteria until 1000 articles have been selected
sample_list = []
while len(sample_list)<1000:
	#draw random batches of 100
	pmids = sample(all_pmids,100)
	print(len(pmids))
	all_tested_ids.extend(pmids)
	reqs = []
	for pmid in pmids:
		request = requests.Request(method='GET', url=f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&api_key=2fb02c88d5ee6cf7f37ce4e1b5b96e664009&id={pmid}&retmode=xml')
		reqs.append(request)
	throttled_requests = []
	with BaseThrottler(name='base-throttler', reqs_over_time=(10,1)) as bt:
		throttled_requests = bt.multi_submit(reqs)
	#rate_limiter = RateLimiter(max_calls=10, period=1)
	#for pmid in pmids:
	#	with rate_limiter:
	#		response = requests.get(f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&api_key=2fb02c88d5ee6cf7f37ce4e1b5b96e664009&{pmid}&retmode=xml')
	#		reqs.append(response)
	articles = []
	for request in throttled_requests:
		#screen out exceptions (unsuccessful fetches)
		if not request.exception:
			articles.append(request.response.content)
		else:
			exception_count += 1
	for article in articles:
		repeat = False
		soup = BS(article,'lxml-xml')
		affiliation = soup.find('Affiliation')
		pmid = soup.find('PMID')
		print(affiliation)
		print(pmid)
		#screen out articles without affiliation fields
		if affiliation:
			#screen out articles already in sample_list
			for sample_article in sample_list:
				if sample_article['pmid'] == pmid.text:
					repeats.append(pmid.text)
					repeat = True
					print("repeat")
					break
			if not repeat:
				entry = {'pmid':pmid.text, 'affiliation':affiliation.text}
				sample_list.append(entry)
				print("added")
		else:
			if pmid:
				no_affiliation.append(pmid.text)
			else:
				no_affiliation.append(soup)
			print("no affiliation")
	print(f"added {len(sample_list)} articles, {len(repeats)} repeats, {len(no_affiliation)} no affiliation, {exception_count} exceptions")


	


#write summary text file
with open('pubmed_random_sample_summary.txt', 'w') as f:
	f.write(f'tested {len(all_tested_ids)} pmids: {all_tested_ids}\n')
	f.write(f'encountered {exception_count} exceptions when fetching\n')
	f.write(f'encountered {len(repeats)} repeats: {repeats}\n')
	f.write(f'pulled {len(no_affiliation)} articles with no affiliation field: {no_affiliation}\n')
	f.write(f'pulled {len(sample_list)} qualified articles: {sample_list}\n')

#write sample to csv
keys = sample_list[0].keys()
with open('pmsample.csv','w',newline='') as output_file:
	dict_writer = csv.DictWriter(output_file, keys)
	dict_writer.writeheader()
	dict_writer.writerows(sample_list)

"""Code to clean up the affiliation fields"""

import pandas as pd
import csv, re, string

# read file from csv
with open('pmsample.csv') as csvfile:
	df = pd.read_csv(csvfile)

email_lists = []
emails = []
new_affiliation = []
exclist = string.punctuation + string.digits + ' '

# find, extract, and remove email addresses, strip punctuation, white space, and numbers from beginning and end of affiliation
for affiliation in df['affiliation']:
	match = re.findall(r'[\w.+-]+@[\w-]+\.[\w.-]+', affiliation)
	stripped_emails = []
	for email in match:
		stripped_emails.append(email.strip(string.punctuation))
	email_lists.append(stripped_emails)
	if match:
		for email in match:
			affiliation = affiliation.replace(email,'')
		print(affiliation)
		affiliation = re.sub('email address','', affiliation, flags=re.IGNORECASE)
		affiliation = re.sub('e-mail address','',affiliation, flags=re.IGNORECASE)
		affiliation = re.sub('electronic address','', affiliation, flags=re.IGNORECASE)
		affiliation = re.sub('email','',affiliation, flags=re.IGNORECASE)
		affiliation = re.sub('e-mail','',affiliation, flags=re.IGNORECASE)
		print(affiliation)
	affiliation = affiliation.strip(exclist)
	if match:
		print(affiliation)
	new_affiliation.append(affiliation)


for email_list in email_lists:
	if email_list:
		emails.append(','.join(email_list))
	else:
		emails.append('')

df['email'] = emails
df['affiliation'] = new_affiliation

#write df to csv file
df.to_csv(f'pmsample_clean.csv')