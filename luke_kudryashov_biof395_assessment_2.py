# -*- coding: utf-8 -*-
"""
This is the pilot proof of concept project, provided here for reference. It was created using Google Colab.

Luke Kudryashov BIOF395 Assessment 2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18D_jSzoEAI0vjZGHqxIHbgDEykXW-4QT

# Text mining problem description

I am working on a Named Entity Recognition project to identify the country of the first author in articles on PubMed. The metadata in PubMed provides author affiliation as a free text field with formatting that varies widely from publisher to publisher. Articles include some or all of the following: institution, city, country, state, email, street address, names. Formatting of locations and institutions is not standardized, making analysis of the geographical distribution of authors in PubMed difficult. Text minining through NER is necessary for large scale geographical analysis. I will also be training the model to recognize cities, states, organizations, and emails to facilitate determining the country when it is not included in the affiliation metadata (for example, the name of the university, the domain of the email address, or the city and state could be matched against databases to determine the country). For the purposes of this project, I will only be completing the first step of identifying the COUNTRY, STATE, CITY, ORGANIZATION, and EMAIL entities. I will be training a spaCy model in order to accomplish this as spaCy is specialized for NER and is efficient, accurate, and adaptable.

Geographical distribution is an important topic to explore, because researchers from countries in the Global South face many barriers to publishing so the concerns and perspectives from those countries are underrepresented in the biomedical literature. The geographic distribution of authors in PubMed can shed light on the extent and contours of this issue in the PubMed database, the largest database of biomedical literature in the world. A more granular analysis by subfield or topic can also yield insight into representation within those specific research areas and into research priorities by country. For this project, I am focusing on first authors to narrow the scope, but this work could be extended to incorporate all authors for whom affiliation metadata is provided.

#Dataset description

I am pulling the dataset from a PubMed search API (eutilities). Since PubMed has over 27 million articles, I have narrowed down the search to articles with the search term "eczema" published in the last two years. This resulted in a dataset of 2,102 records. The field that is of interest to my analysis is the first AD (Affiliation) field - the affiliation info for the first author. 

I have only had time to manually annotate 301 records, so I will be using this set of 301 to train, validate, and test the model. The model could be trained with a higher degree of accuracy in the future with a larger annotated dataset. I annotated the dataset using https://manivannanmurugavel.github.io/annotating-tool/spacy-ner-annotator/, resulting in a JSON file that can be translated into the spaCy training data format. The annotated data includes the text for each record along with the entity labels and start and end indices for each label.

#Text processing

The code I used to pull data from PubMed and save it to a CSV file is below. Note that I pulled the data I later annotated on 12/5/2021. If this code is rerun, the dataset that results will be different, since the query is fetching articles published in the last year.
"""

import requests

# get PMIDs for PubMed search results for "eczema" in the last year through eutilities
response = requests.get('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=eczema&reldate=730&datetype=pdat&retmax=2218&retmode=json')
ids = response.json()['esearchresult']['idlist']
print(f"fetched {len(ids)} ids")

print(f"fetched {len(throttled_requests)} articles")
# take XML metadata from fetched throttled requests
articles = []
for request in throttled_requests:
# screen out exceptions (unsuccessful fetches)
  if not request.exception:
    articles.append(request.response.content)
print(f"added metadata from {len(articles)} articles")

#import Beautiful Soup for easier XML processing
from bs4 import BeautifulSoup as BS
# for each article, keep only the PMID and first <Affiliation> tag
data = []
#keep track of the articles that don't have an <Affiliation> tag
bin = []
for article in articles:
  soup = BS(article,'xml')
  affiliation = soup.find('Affiliation')
  pmid = soup.find('PMID')
  if affiliation and pmid:
    entry = {'pmid': pmid.text, 'affiliation': affiliation.text}
    data.append(entry)
  else:
    bin.append(article)
print(f"{len(data)} articles with affiliation")  
print(f"{len(bin)} articles with no affiliation")

# fetch XML metadata for each PMID in ids

#set up a throttle to abide by 10 requests/ second eutilities API rate limit. referenced code from https://stackoverflow.com/questions/20247354/limiting-throttling-the-rate-of-http-requests-in-grequests
#import throttler
!pip install RequestsThrottler==0.2.2
from requests_throttler import BaseThrottler

#set up list of requests
reqs = []
for id in ids:
  request = requests.Request(method='GET', url=f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&api_key=2fb02c88d5ee6cf7f37ce4e1b5b96e664009&id={id}&retmode=xml")
  reqs.append(request)
#pass requests through throttler at a rate of 10 per second
throttled_requests = []
with BaseThrottler(name='base-throttler', reqs_over_time=(10, 1)) as bt:
  throttled_requests = bt.multi_submit(reqs)

#save data as csv file. Referenced https://stackoverflow.com/questions/3086973/how-do-i-convert-this-list-of-dictionaries-to-a-csv-file
from google.colab import files
import csv

keys = data[0].keys()

with open('pmdata.csv', 'w', newline='') as output_file:
  dict_writer = csv.DictWriter(output_file, keys)
  dict_writer.writeheader()
  dict_writer.writerows(data)

#download data
files.download('pmdata.csv')

"""After downloading the data as a CSV file, I annotated the entities in the affiliation fields of the first 301 records. I did not perform other text processing because elements such as the order of words, punctuation, capitalization, and even stop words like "of" and "at" all serve as important indicators of entity boundaries and types of entities, so I wanted to leave as much of that information in as possible.

#Text representation
"""

import json
import random
from google.colab import files

# upload annotated data (json format)
# I split the data into 3 files of 100 records each for easier annotation
# upload and convert each dataset separately, then merge and shuffle

# upload df1.json
print("Please upload df1.json")
uploaded = files.upload()

# convert json to spacy format (https://github.com/ManivannanMurugavel/spacy-ner-annotator/blob/master/convert_spacy_train_data.py)

with open('df1.json') as train_data:
	train = json.load(train_data)

annotated_data = []
for data in train:
	ents = [tuple(entity[:3]) for entity in data['entities']]
	annotated_data.append((data['content'],{'entities':ents}))
 
print(f"Added records from df1. Length: {len(annotated_data)}")

# upload df2.json
print("Please upload df2.json")
uploaded = files.upload()

# convert json to spacy format 

with open('df2.json') as train_data:
	train = json.load(train_data)

for data in train:
	ents = [tuple(entity[:3]) for entity in data['entities']]
	annotated_data.append((data['content'],{'entities':ents}))

print(f"Added records from df2. Length: {len(annotated_data)}")

# upload df3.json
print("Please upload df3.json")
uploaded = files.upload()

# convert json to spacy format 

with open('df3.json') as train_data:
	train = json.load(train_data)

for data in train:
	ents = [tuple(entity[:3]) for entity in data['entities']]
	annotated_data.append((data['content'],{'entities':ents}))

print(f"Added records from df3. Length: {len(annotated_data)}")

#not sure why, but the random seed wasn't working until I moved the annotated data over into a new list
data = annotated_data[:301]

#shuffle annotated_data
#set a seed for reproducibility
random.seed(30)
random.shuffle(data)

#calculate training, valid, and test sizes
train_size = int(len(data) * 0.7)
valid_size = int(len(data) * 0.1)
test_size = len(data) - train_size - valid_size

#split the tuple list into training, valid, and test sets
TRAINING_DATA = data[:train_size]
VALID_DATA = data[train_size:train_size+valid_size]
TEST_DATA = data[train_size+valid_size:]

print(TRAINING_DATA)
print(VALID_DATA)
print(TEST_DATA)
print(f"Annotated data length: {len(data)}")
print(f"Training data length: {len(TRAINING_DATA)}")
print(f"Validation data length: {len(VALID_DATA)}")
print(f"Test data length: {len(TEST_DATA)}")

"""Confirmed that data is in the correct spaCy format and split among training, validation, and test sets at the right ratio."""

import spacy 

#create a new spacy model
nlp = spacy.blank('en')

#create a new entity recognizer and add it to the pipeline
ner = nlp.create_pipe('ner')
nlp.add_pipe(ner)

#add the label 'COUNTRY' to the entity recognizer
ner.add_label('COUNTRY')

#add the label 'STATE' to the entity recognizer
ner.add_label('STATE')

#add the label 'CITY' to the entity recognizer
ner.add_label('CITY')

#add the label 'ORGANIZATION' to the entity recognizer
ner.add_label('ORGANIZATION')

#add the label 'EMAIL' to the entity recognizer
ner.add_label('EMAIL')

#training data will be run through this pipeline

"""Set up the spaCy pipeline - including 'ner' that will be used for text processing when the model runs.

#Text models
"""

import random

# start the training on the nlp model created earlier (code adapted from datacamp)
optimizer = nlp.begin_training()

# loop for 10 iterations
for itn in range(10):
    # shuffle the training data
    random.seed(7)
    random.shuffle(TRAINING_DATA)
    losses = {}
    
    # batch the examples and iterate over them
    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):
        texts = [text for text, entities in batch]
        annotations = [entities for text, entities in batch]
        
        # update the model
        nlp.update(texts, annotations, losses=losses)
        print(losses)

"""This step completes the initial training of the model using the TRAINING_DATA. Batches and iterations are used to train the model on multiple combinations of the data. No drop rate to begin - that will be adjusted later during validation. A random seed is set for reproducibility.

#Evaluations
"""

# validate the trained model (adapted from: stackoverflow.com/questions/50644777/understanding-spacys-scorer-output)
from spacy.gold import GoldParse
from spacy.scorer import Scorer

scorer = Scorer()
try:
  for input_, annot in VALID_DATA:
    doc_gold_text = nlp.make_doc(input_)
    gold = GoldParse(doc_gold_text, entities=annot['entities'])
    pred_value = nlp(input_)
    scorer.score(pred_value, gold)
except Exception as e: print(e)

print(scorer.scores)

"""I am validating the model using the spaCy scorer, which gives me the overall precision, recall, and f-score across all entities as well as individual precision, recall, and f-scores for each entity. These scores allow me to assess how well the model is performing overall as well as on each entity and make adjustment accordingly. I am most concerned about having high scores for COUNTRY, especially high precision, since if country information is available, I want it to be identified.

Fairly good performance - overall precision 85, recall 89, f-score 87. COUNTRY has higher recall (96) than precision (88). Organization has slightly lower precision (83) than recall (85). CITY has equal precision and recall (93). STATE has the lowest precision at 67 and a higher but still fairly low recall at 80. EMAIL has perfect recall (100) but lower precision (80).
"""

# try modifying training configuration: add drop rate of .2
for itn in range(10):
    random.seed(9)
    random.shuffle(TRAINING_DATA)
    losses = {}
  
    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):
      texts = [text for text, entities in batch]
      annotations = [entities for text, entities in batch]    
      nlp.update(texts, annotations, drop=.2, losses=losses)

Added a drop rate since that can help to increase model accuracy.

# validate
scorer = Scorer()
try:
  for input_, annot in VALID_DATA:
    doc_gold_text = nlp.make_doc(input_)
    gold = GoldParse(doc_gold_text, entities=annot['entities'])
    pred_value = nlp(input_)
    scorer.score(pred_value, gold)
except Exception as e: print(e)

print(scorer.scores)

"""Overall precision, recall, and f-score have gone up slightly to 88, 91, and 89 respectively. COUNTRY precision has gone up. ORGANIZATION precision and recall have both gone up. CITY precision and recall have slightly gone down. STATE precision has gone down. EMAIL precision has gone up. Since overall scores and COUNTRY scores have both gone up, this configuration is more successful for the goal of optimizing COUNTRY detection."""

# test the trained model
scorer = Scorer()
try:
  for input_, annot in TEST_DATA:
    doc_gold_text = nlp.make_doc(input_)
    gold = GoldParse(doc_gold_text, entities=annot['entities'])
    pred_value = nlp(input_)
    scorer.score(pred_value, gold)
except Exception as e: print(e)

print(scorer.scores)

"""Test data scores are similar to validation scores. Overall precision, recall, and f-scores are slightly lower but still good (84, 89, 87). COUNTRY scores are the best out of all the entities and even higher than they were for the validation data (95, 98, 96) - a good result, since COUNTRY detection is the key goal of this model. CITY and STATE scores did drop but those entities are of secondary importance.

#Observations

Describe (1) what you have learnt in this project, (2) what are strengths and limitations of your text mining pipelines and (3) any future work that could further improve your pipeline

**Lessons.**

In this project, I have learned how to use E-Utilities to pull article metadata from PubMed searches, how to annotate that data to prepare it for NLP training, and how to create, tune, and evaluate a custom NER model with spaCy. I have developed a custom model that will serve as a starting point for a group project I am working on to assess the diversity (including geographic diversity) of the PubMed database.

**Strenghts and Limitations.**

This model performs well overall and especially well for detecting countries, which is its main purpose. It does not perform as well at detecting the other entities, especially STATE and to a lesser extent CITY.

Lower precision of STATE and low initial precision of EMAIL are likely because fewer records included states than other entitites. States may also be harder to detect than cities and countries because they are more often skipped in affiliation metadata - it is more common to see the format "CITY, COUNTRY" than "STATE, COUNTRY".

**Future Work.**

When annotating, I ignored hyperspecific location information such as neighborhoods. However, leaving out these entities may have introduced more error. In future work, I could add annotations for NEIGHBORHOOD to see if this improves performance.

Punctuation in email strings may also confuse the spaCy parser. Future work can improve the precision of email detection through pattern matching instead of machine learning since email addresses follow a predictable pattern.

Providing a larger annotated training data set would also likely improve the precision and recall of the model.

To help with determining country for records in which the country was not explicitly specified, I could also train the model to differentiate between DEPARTMENT and INSTITUTION (both currently annotated as ORGANIZATION), since institution name would be much more helpful in determining the affiliated country than the department name is.

Additional work needs to be done in standardizing the entity formats once they are detected - for example, some countries and states are written out fully while others are abbreviated, and ocassionally some are in a different language.
"""
